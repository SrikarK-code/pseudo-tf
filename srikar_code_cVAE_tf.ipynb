{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "98wyaIenVRVy",
        "6RY6YHYjsF1n",
        "2V0zbcYArz8L",
        "ThFYn2_Lr2_s",
        "tOW-JTYbqgaA",
        "MDJunl7Dqgcu"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98wyaIenVRVy"
      },
      "source": [
        "\n",
        "\n",
        "## Mount Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlFIq2zSCcrx",
        "outputId": "7a10d23e-a694-4aeb-8d0d-9017012ee247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nq17nt0ACxOY"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lk22LUvbCygH"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/tf design')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RY6YHYjsF1n"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8ZJ-zQ_-sAo3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nabimZ4gEadt",
        "outputId": "fbbbfaec-cf84-421d-81bb-c88b127aa40e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BohHOWDpsAo5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "def pad_tensor(tensor, max_length, dim=0):\n",
        "    tensor = torch.tensor(tensor) if isinstance(tensor, np.ndarray) else tensor\n",
        "    pad_size = max_length - tensor.shape[dim]\n",
        "    padding = (0, 0) * (tensor.dim() - dim - 1) + (0, pad_size)\n",
        "    return torch.nn.functional.pad(tensor, padding)\n",
        "\n",
        "class CellProteinDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, joint_embeddings, esm_embeddings):\n",
        "        assert len(joint_embeddings) == len(esm_embeddings), \"Dictionaries must have the same length\"\n",
        "\n",
        "        self.joint_keys = list(joint_embeddings.keys())\n",
        "        self.esm_keys = list(esm_embeddings.keys())\n",
        "\n",
        "        self.joint_embeddings = joint_embeddings\n",
        "        self.esm_embeddings = esm_embeddings\n",
        "\n",
        "        # Compute the maximum embedding size\n",
        "        self.max_joint_dim = max(tensor.shape[0] for tensor in joint_embeddings.values())\n",
        "        self.max_esm_dim = max(tensor.shape[0] for tensor in esm_embeddings.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.joint_embeddings)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        joint_key = self.joint_keys[index]\n",
        "        esm_key = self.esm_keys[index]\n",
        "\n",
        "        joint_embedding = pad_tensor(self.joint_embeddings[joint_key], self.max_joint_dim)\n",
        "        esm_embedding = pad_tensor(self.esm_embeddings[esm_key], self.max_esm_dim)\n",
        "\n",
        "        return {\n",
        "            \"cell_input\": joint_embedding,\n",
        "            \"protein_input\": esm_embedding\n",
        "        }\n",
        "\n",
        "class CellProteinCollator:\n",
        "    def __call__(self, raw_batch):\n",
        "        batch = {}\n",
        "\n",
        "        cell_input_list = [v['cell_input'] for v in raw_batch]\n",
        "        protein_input_list = [v['protein_input'] for v in raw_batch]\n",
        "\n",
        "        batch['cell_input'] = torch.stack(cell_input_list)\n",
        "        batch['protein_input'] = torch.stack(protein_input_list)\n",
        "\n",
        "        return batch\n",
        "\n",
        "class CellProteinDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, joint_embeddings, esm_embeddings, batch_size):\n",
        "        super().__init__()\n",
        "\n",
        "        dataset = CellProteinDataset(joint_embeddings, esm_embeddings)\n",
        "\n",
        "        # Splitting data into train, test, and validation\n",
        "        train_size = int(0.7 * len(dataset))\n",
        "        val_size = int(0.15 * len(dataset))\n",
        "        test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.collator = CellProteinCollator()\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collator, shuffle=True,drop_last=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        full_batch = DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collator, shuffle=False,drop_last=True)\n",
        "        #binary_batch = DataLoader(self.val_dataset, batch_size=2, collate_fn=self.collator, shuffle=False)\n",
        "        return [full_batch]#, binary_batch]\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=self.collator, shuffle=False,drop_last=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yXY6j_BWukFi"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-hjeb7Gmjz5Y"
      },
      "outputs": [],
      "source": [
        "class CellProteinDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, joint_embeddings, esm_embeddings, batch_size, train_keys_list=None, val_test_keys_list=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Subset dictionaries based on provided keys\n",
        "        train_joint_embeddings = {key: joint_embeddings[key] for key in train_keys_list} if train_keys_list else joint_embeddings\n",
        "        train_esm_embeddings = {key: esm_embeddings[key] for key in train_keys_list} if train_keys_list else esm_embeddings\n",
        "\n",
        "        val_test_joint_embeddings = {key: joint_embeddings[key] for key in val_test_keys_list} if val_test_keys_list else joint_embeddings\n",
        "        val_test_esm_embeddings = {key: esm_embeddings[key] for key in val_test_keys_list} if val_test_keys_list else esm_embeddings\n",
        "\n",
        "        # Create datasets\n",
        "        self.train_dataset = CellProteinDataset(train_joint_embeddings, train_esm_embeddings)\n",
        "        val_test_dataset = CellProteinDataset(val_test_joint_embeddings, val_test_esm_embeddings)\n",
        "\n",
        "        # Splitting val_test_dataset into validation and test sets\n",
        "        val_size = int(0.15 * len(val_test_dataset))\n",
        "        test_size = len(val_test_dataset) - val_size\n",
        "\n",
        "        self.val_dataset, self.test_dataset = random_split(val_test_dataset, [val_size, test_size])\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.collator = CellProteinCollator()\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collator, shuffle=True,drop_last=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        full_batch = DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collator, shuffle=False,drop_last=True)\n",
        "        binary_batch = DataLoader(self.val_dataset, batch_size=2, collate_fn=self.collator, shuffle=False)\n",
        "        return [full_batch,binary_batch]\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=self.collator, shuffle=False,drop_last=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAFKJrvuFQ-V"
      },
      "source": [
        "#### load norman19 dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oAuhwzYG5UWi"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_subset.pkl', 'rb') as f:\n",
        "    train_dataset_norman = pickle.load(f)\n",
        "\n",
        "with open('test_subset.pkl', 'rb') as f:\n",
        "    val_dataset_norman = pickle.load(f)\n",
        "\n",
        "with open('val_subset.pkl', 'rb') as f:\n",
        "    test_dataset_norman = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "0WU6nUacCCEn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TatpfRXFCpxd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "train_indices = list(range(2500))\n",
        "test_indices = list(range(1000))\n",
        "val_indices = list(range(500))\n",
        "\n",
        "# Create subsets\n",
        "train_subset = Subset(train_dataset_norman, train_indices)\n",
        "test_subset = Subset(test_dataset_norman, test_indices)\n",
        "val_subset = Subset(val_dataset_norman, val_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpeCwfCqCRbF",
        "outputId": "986d8b45-2156-4846-e118-23c918979345"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2500"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(train_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjvz9VD5CZhh",
        "outputId": "fb997470-a875-47fe-c937-8e914800b9cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "type(train_dataset_norman)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lWxfhFjA5Y8i"
      },
      "outputs": [],
      "source": [
        "with open('dataloader_params.pkl', 'rb') as f:\n",
        "    dataloader_params = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccf3eXBgOY-R",
        "outputId": "8ffa6a0f-270a-489b-ef8f-4c9a148775ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'batch_size': 4, 'shuffle': 'True'},\n",
              " 'val': {'batch_size': 4, 'shuffle': 'False'},\n",
              " 'test': {'batch_size': 4, 'shuffle': 'False'}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "dataloader_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iQjRaZo15cGX"
      },
      "outputs": [],
      "source": [
        "train_loader_norman = DataLoader(train_subset, **dataloader_params['train'])\n",
        "val_loader_norman = DataLoader(val_subset, **dataloader_params['val'])\n",
        "test_loader_norman = DataLoader(test_subset, **dataloader_params['test'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "W1qHiXCHryXq"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WAldxnKa5hcR"
      },
      "outputs": [],
      "source": [
        "# Converting string 'True'/'False' to boolean\n",
        "dataloader_params['train']['shuffle'] = dataloader_params['train']['shuffle'] == 'True'\n",
        "dataloader_params['val']['shuffle'] = dataloader_params['val']['shuffle'] == 'True'\n",
        "dataloader_params['test']['shuffle'] = dataloader_params['test']['shuffle'] == 'True'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FuI1f0BzzpwN"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vTkaVcdOPxJX"
      },
      "outputs": [],
      "source": [
        "esm_dataset = train_loader_norman.dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx7aBSUYzfLd",
        "outputId": "eb619396-5d3b-4821-f047-e9ef0d99bede"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2078, 1280])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "esm_dataset[1]['protein_input'].shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "esm_dataset[1]['cell_input'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ejj6J6RB15g",
        "outputId": "84102728-60a8-47da-a6a9-fe9ebb10f9fe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8350, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNIOaZbX0XBQ",
        "outputId": "4b6332a1-5ad8-4324-f1cc-d9f5356ea853"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cell_input': tensor([[-1.5663,  1.1364,  0.6010,  ...,  0.1193, -0.1001,  0.0477],\n",
              "         [-1.3502,  0.4448,  1.6567,  ...,  0.1193, -0.1001,  0.0477],\n",
              "         [ 0.0071,  0.2452, -0.1685,  ...,  0.1193, -0.1001,  0.0477],\n",
              "         ...,\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
              " 'protein_input': tensor([[-0.0160, -0.1942,  0.1267,  ...,  0.0727, -0.1567,  0.1401],\n",
              "         [ 0.0547,  0.1077,  0.1109,  ...,  0.0577,  0.0728,  0.1995],\n",
              "         [ 0.0195,  0.0153,  0.2484,  ..., -0.0064,  0.1929, -0.0309],\n",
              "         ...,\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "esm_dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "RphZC3Khv3ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cVAE-no-attn"
      ],
      "metadata": {
        "id": "iBfkQV-6qcA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "a9UCmYWeqB8b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConditionalVAE(nn.Module):\n",
        "    def __init__(self, hidden_dim=512, latent_dim=256, joint_embedding_dim=20, esm_embedding_dim=1280, dropout_rate=0.5, beta=1):\n",
        "        super(ConditionalVAE, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.fc_encode1 = nn.Linear(joint_embedding_dim*8350, hidden_dim)\n",
        "        self.fc_encode1_bn = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc_encode_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_encode_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc_decode1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc_decode1_bn = nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc_decode2 = nn.Linear(hidden_dim, esm_embedding_dim)\n",
        "\n",
        "        # Final reshaping layer\n",
        "        self.final_reshape = nn.Linear(esm_embedding_dim, 2078 * esm_embedding_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "        self.beta = beta\n",
        "\n",
        "    def encode(self, joint_embedding):\n",
        "        h1 = self.leaky_relu(self.fc_encode1(joint_embedding))\n",
        "        h1 = self.fc_encode1_bn(h1)\n",
        "        h1 = self.dropout(h1)\n",
        "        return self.fc_encode_mu(h1), self.fc_encode_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h2 = self.leaky_relu(self.fc_decode1(z))\n",
        "        h2 = self.fc_decode1_bn(h2)\n",
        "        h2 = self.dropout(h2)\n",
        "        return self.fc_decode2(h2)\n",
        "\n",
        "    def forward(self, joint_embedding):\n",
        "        # Flatten joint_embedding for processing\n",
        "        joint_embedding = joint_embedding.view(joint_embedding.size(0), -1)\n",
        "        mu, logvar = self.encode(joint_embedding)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        generated_esm_embedding = self.decode(z)\n",
        "        print('after decode generated_esm_embedding shape', generated_esm_embedding.shape)\n",
        "        generated_esm_embedding = self.final_reshape(generated_esm_embedding)\n",
        "        generated_esm_embedding = generated_esm_embedding.view(-1, 2078, 1280)\n",
        "        print('generated_esm_embedding shape', generated_esm_embedding.shape)\n",
        "        return generated_esm_embedding, mu, logvar\n",
        "\n",
        "    def loss_function(self, generated_esm_embedding, real_esm_embedding, mu, logvar):\n",
        "        MSE = F.mse_loss(generated_esm_embedding, real_esm_embedding, reduction='sum')\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return MSE + self.beta * KLD, MSE, KLD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cVAE-rnn-no-attn"
      ],
      "metadata": {
        "id": "2V0zbcYArz8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConditionalVAERNN(nn.Module):\n",
        "    def __init__(self, hidden_dim=512, latent_dim=256, joint_embedding_dim=20, esm_embedding_dim=1280, rnn_hidden_dim=256, num_layers=2, dropout_rate=0.5, beta=1):\n",
        "        super(ConditionalVAERNN, self).__init__()\n",
        "\n",
        "        # RNN Encoder\n",
        "        self.rnn_encoder = nn.RNN(joint_embedding_dim, rnn_hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
        "        self.fc_encode_mu = nn.Linear(rnn_hidden_dim, latent_dim)\n",
        "        self.fc_encode_logvar = nn.Linear(rnn_hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc_decode1 = nn.Linear(latent_dim, rnn_hidden_dim)\n",
        "        self.rnn_decoder = nn.RNN(rnn_hidden_dim, esm_embedding_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
        "\n",
        "        # Final reshaping layer\n",
        "        self.final_reshape = nn.Linear(esm_embedding_dim, 2078 * esm_embedding_dim)\n",
        "\n",
        "        self.beta = beta\n",
        "\n",
        "    def encode(self, joint_embedding):\n",
        "        _, h_n = self.rnn_encoder(joint_embedding)  # Use the last hidden state\n",
        "        h_n = h_n[-1]  # Take the last layer's hidden state, shape: [batch_size, rnn_hidden_dim]\n",
        "        return self.fc_encode_mu(h_n), self.fc_encode_logvar(h_n)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = z.unsqueeze(1).repeat(1, 2078, 1)  # Repeat z for each time step\n",
        "        output, _ = self.rnn_decoder(z)\n",
        "        return output\n",
        "\n",
        "    def forward(self, joint_embedding):\n",
        "        joint_embedding = joint_embedding.view(joint_embedding.size(0), -1, 20)\n",
        "        mu, logvar = self.encode(joint_embedding)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        print('z (reparameterize) shape:',z.shape)\n",
        "        generated_esm_embedding = self.decode(z)\n",
        "        print('after decode generated_esm_embedding shape', generated_esm_embedding.shape)\n",
        "        return generated_esm_embedding, mu, logvar\n",
        "\n",
        "    def loss_function(self, generated_esm_embedding, real_esm_embedding, mu, logvar):\n",
        "        MSE = F.mse_loss(generated_esm_embedding, real_esm_embedding, reduction='sum')\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return MSE + self.beta * KLD, MSE, KLD\n"
      ],
      "metadata": {
        "id": "8wPXbMnmr3yb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cVAE-lstm-no-attn"
      ],
      "metadata": {
        "id": "ThFYn2_Lr2_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConditionalVAELSTM(nn.Module):\n",
        "    def __init__(self, hidden_dim=512, latent_dim=256, joint_embedding_dim=20, esm_embedding_dim=1280, lstm_hidden_dim=256, num_layers=2, dropout_rate=0.5, beta=1):\n",
        "        super(ConditionalVAELSTM, self).__init__()\n",
        "\n",
        "        # LSTM Encoder\n",
        "        self.lstm_encoder = nn.LSTM(joint_embedding_dim, lstm_hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
        "        self.fc_encode_mu = nn.Linear(lstm_hidden_dim, latent_dim)\n",
        "        self.fc_encode_logvar = nn.Linear(lstm_hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc_decode1 = nn.Linear(latent_dim, lstm_hidden_dim)\n",
        "        self.lstm_decoder = nn.LSTM(lstm_hidden_dim, esm_embedding_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
        "\n",
        "        # Final reshaping layer\n",
        "        self.final_reshape = nn.Linear(esm_embedding_dim, 2078 * esm_embedding_dim)\n",
        "\n",
        "        self.beta = beta\n",
        "\n",
        "    def encode(self, joint_embedding):\n",
        "        _, (h_n, _) = self.lstm_encoder(joint_embedding)  # Use the last hidden state\n",
        "        h_n = h_n[-1]  # Take the hidden state from the last LSTM layer\n",
        "        return self.fc_encode_mu(h_n), self.fc_encode_logvar(h_n)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = z.unsqueeze(1).repeat(1, 2078, 1)  # Repeat z for each time step\n",
        "        output, _ = self.lstm_decoder(z)\n",
        "        return output\n",
        "\n",
        "    def forward(self, joint_embedding):\n",
        "        joint_embedding = joint_embedding.view(joint_embedding.size(0), -1, 20)\n",
        "        mu, logvar = self.encode(joint_embedding)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        generated_esm_embedding = self.decode(z)\n",
        "        print('generated_esm_embedding shape', generated_esm_embedding.shape)\n",
        "        return generated_esm_embedding, mu, logvar\n",
        "\n",
        "    def loss_function(self, generated_esm_embedding, real_esm_embedding, mu, logvar):\n",
        "        MSE = F.mse_loss(generated_esm_embedding, real_esm_embedding, reduction='sum')\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return MSE + self.beta * KLD, MSE, KLD\n"
      ],
      "metadata": {
        "id": "57zIIlOYr6dH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cVAE-rnn-attn"
      ],
      "metadata": {
        "id": "tOW-JTYbqgaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNNAttentionEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_rate, attention_heads):\n",
        "        super(RNNAttentionEncoder, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
        "        self.attention = nn.MultiheadAttention(hidden_dim, attention_heads, batch_first=True)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.rnn(x)\n",
        "        attn_output, _ = self.attention(output, output, output)\n",
        "        return attn_output.mean(dim=1)\n",
        "\n",
        "class ConditionalVAERNNAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim=512, latent_dim=256, joint_embedding_dim=20, esm_embedding_dim=1280, rnn_hidden_dim=256, num_layers=2, dropout_rate=0.5, beta=1, attention_heads=4):\n",
        "        super(ConditionalVAERNNAttention, self).__init__()\n",
        "\n",
        "        # RNN Attention Encoder\n",
        "        self.rnn_attention_encoder = RNNAttentionEncoder(joint_embedding_dim, rnn_hidden_dim, num_layers, dropout_rate, attention_heads)\n",
        "        self.fc_encode_mu = nn.Linear(rnn_hidden_dim, latent_dim)\n",
        "        self.fc_encode_logvar = nn.Linear(rnn_hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc_decode1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc_decode2 = nn.Linear(hidden_dim, esm_embedding_dim)\n",
        "\n",
        "        # Final reshaping layer\n",
        "        self.final_reshape = nn.Linear(esm_embedding_dim, 2078 * esm_embedding_dim)\n",
        "\n",
        "        self.beta = beta\n",
        "\n",
        "    def encode(self, joint_embedding):\n",
        "        attn_output = self.rnn_attention_encoder(joint_embedding)\n",
        "        return self.fc_encode_mu(attn_output), self.fc_encode_logvar(attn_output)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h2 = F.relu(self.fc_decode1(z))\n",
        "        h2 = self.fc_decode2(h2)\n",
        "        return h2\n",
        "\n",
        "    def forward(self, joint_embedding):\n",
        "        joint_embedding = joint_embedding.view(joint_embedding.size(0), -1, 20)\n",
        "        mu, logvar = self.encode(joint_embedding)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        generated_esm_embedding = self.decode(z)\n",
        "        print('after decode generated_esm_embedding shape', generated_esm_embedding.shape)\n",
        "        generated_esm_embedding = self.final_reshape(generated_esm_embedding)\n",
        "        generated_esm_embedding = generated_esm_embedding.view(-1, 2078, 1280)\n",
        "        print('generated_esm_embedding shape', generated_esm_embedding.shape)\n",
        "        return generated_esm_embedding, mu, logvar\n",
        "\n",
        "    def loss_function(self, generated_esm_embedding, real_esm_embedding, mu, logvar):\n",
        "        MSE = F.mse_loss(generated_esm_embedding, real_esm_embedding, reduction='sum')\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return MSE + self.beta * KLD, MSE, KLD\n"
      ],
      "metadata": {
        "id": "ctH9HYn8uKvx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cVAE-lstm-attn"
      ],
      "metadata": {
        "id": "MDJunl7Dqgcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMAttentionEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_rate, attention_heads):\n",
        "        super(LSTMAttentionEncoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
        "        self.attention = nn.MultiheadAttention(hidden_dim, attention_heads, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.lstm(x)\n",
        "        attn_output, _ = self.attention(output, output, output)\n",
        "        return attn_output.mean(dim=1)\n",
        "\n",
        "class ConditionalVAELSTMAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim=512, latent_dim=256, joint_embedding_dim=20, esm_embedding_dim=1280, lstm_hidden_dim=256, num_layers=2, dropout_rate=0.5, beta=1, attention_heads=4):\n",
        "        super(ConditionalVAELSTMAttention, self).__init__()\n",
        "\n",
        "        # LSTM Attention Encoder\n",
        "        self.lstm_attention_encoder = LSTMAttentionEncoder(joint_embedding_dim, lstm_hidden_dim, num_layers, dropout_rate, attention_heads)\n",
        "        self.fc_encode_mu = nn.Linear(lstm_hidden_dim, latent_dim)\n",
        "        self.fc_encode_logvar = nn.Linear(lstm_hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc_decode1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc_decode2 = nn.Linear(hidden_dim, esm_embedding_dim)\n",
        "\n",
        "        # Final reshaping layer\n",
        "        self.final_reshape = nn.Linear(esm_embedding_dim, 2078 * esm_embedding_dim)\n",
        "\n",
        "        self.beta = beta\n",
        "\n",
        "    def encode(self, joint_embedding):\n",
        "        attn_output = self.lstm_attention_encoder(joint_embedding)\n",
        "        return self.fc_encode_mu(attn_output), self.fc_encode_logvar(attn_output)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h2 = F.relu(self.fc_decode1(z))\n",
        "        h2 = self.fc_decode2(h2)\n",
        "        return h2\n",
        "\n",
        "    def forward(self, joint_embedding):\n",
        "        joint_embedding = joint_embedding.view(joint_embedding.size(0), -1, 20)\n",
        "        mu, logvar = self.encode(joint_embedding)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        generated_esm_embedding = self.decode(z)\n",
        "        print('after decode generated_esm_embedding shape', generated_esm_embedding.shape)\n",
        "        generated_esm_embedding = self.final_reshape(generated_esm_embedding)\n",
        "        generated_esm_embedding = generated_esm_embedding.view(-1, 2078, 1280)\n",
        "        print('generated_esm_embedding shape', generated_esm_embedding.shape)\n",
        "        return generated_esm_embedding, mu, logvar\n",
        "\n",
        "    def loss_function(self, generated_esm_embedding, real_esm_embedding, mu, logvar):\n",
        "        MSE = F.mse_loss(generated_esm_embedding, real_esm_embedding, reduction='sum')\n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return MSE + self.beta * KLD, MSE, KLD\n"
      ],
      "metadata": {
        "id": "7CmZ5PituVdZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Models"
      ],
      "metadata": {
        "id": "dtBraUmU9b_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define all model classes (ConditionalVAE, ConditionalVAERNNAttention, etc.)\n",
        "\n",
        "# Define the init_weights function\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "# Model configurations\n",
        "model_configs = [\n",
        "    {\"class\": ConditionalVAE, \"params\": {\"hidden_dim\": 512, \"latent_dim\": 256, \"joint_embedding_dim\": 20, \"esm_embedding_dim\": 1280, \"dropout_rate\": 0.5, \"beta\": 1}},\n",
        "    {\"class\": ConditionalVAERNN, \"params\": {\"hidden_dim\": 512, \"latent_dim\": 256, \"joint_embedding_dim\": 20, \"esm_embedding_dim\": 1280, \"rnn_hidden_dim\": 256, \"num_layers\": 2, \"dropout_rate\": 0.5, \"beta\": 1}},\n",
        "    {\"class\": ConditionalVAELSTM, \"params\": {\"hidden_dim\": 512, \"latent_dim\": 256, \"joint_embedding_dim\": 20, \"esm_embedding_dim\": 1280, \"lstm_hidden_dim\": 256, \"num_layers\": 2, \"dropout_rate\": 0.5, \"beta\": 1}},\n",
        "    {\"class\": ConditionalVAERNNAttention, \"params\": {\"hidden_dim\": 512, \"latent_dim\": 256, \"joint_embedding_dim\": 20, \"esm_embedding_dim\": 1280, \"rnn_hidden_dim\": 256, \"num_layers\": 2, \"dropout_rate\": 0.5, \"beta\": 1, \"attention_heads\": 4}},\n",
        "    {\"class\": ConditionalVAELSTMAttention, \"params\": {\"hidden_dim\": 512, \"latent_dim\": 256, \"joint_embedding_dim\": 20, \"esm_embedding_dim\": 1280, \"lstm_hidden_dim\": 256, \"num_layers\": 2, \"dropout_rate\": 0.5, \"beta\": 1, \"attention_heads\": 4}}]\n",
        "\n",
        "\n",
        "# Define the function for saving checkpoints\n",
        "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "# Define your data loaders\n",
        "# train_loader_norman, val_loader_norman, test_loader_norman should be defined\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Initialize and train models\n",
        "losses = []\n",
        "\n",
        "for config in model_configs:\n",
        "    # Initialize model, optimizer, and scheduler\n",
        "    print(config['class'])\n",
        "    model = config[\"class\"](**config[\"params\"]).to(device)\n",
        "    model.apply(init_weights)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "    # Initialize best validation loss for checkpointing\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Train on a single batch\n",
        "    model.train()\n",
        "    train_batch = next(iter(train_loader_norman))\n",
        "    joint_embedding, esm_embedding = train_batch['cell_input'].to(device), train_batch['protein_input'].to(device)\n",
        "    optimizer.zero_grad()\n",
        "    recon_emb, mu, logvar = model(joint_embedding)\n",
        "    loss, MSE, KLD = model.loss_function(recon_emb, esm_embedding, mu, logvar)\n",
        "    print('loss:',loss)\n",
        "    print('MSE:',MSE)\n",
        "    print('KLD:',KLD)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation on a single batch\n",
        "    model.eval()\n",
        "    val_batch = next(iter(val_loader_norman))\n",
        "    joint_embedding, esm_embedding = val_batch['cell_input'].to(device), val_batch['protein_input'].to(device)\n",
        "    with torch.no_grad():\n",
        "        recon_emb, mu, logvar = model(joint_embedding)\n",
        "        val_loss, _, _ = model.loss_function(recon_emb, esm_embedding, mu, logvar)\n",
        "        print('val loss:',val_loss)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Checkpointing\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        save_checkpoint({\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "            'best_val_loss': best_val_loss,\n",
        "        }, filename=f\"checkpoint_{config['class'].__name__}.pth.tar\")\n",
        "\n",
        "    # Record losses\n",
        "    losses.append({\n",
        "        'model': config['class'].__name__,\n",
        "        'train_loss': loss.item(),\n",
        "        'val_loss': val_loss\n",
        "    })\n",
        "\n",
        "    # Clear memory\n",
        "    del model, optimizer, scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Print summary\n",
        "for loss_record in losses:\n",
        "    print(f\"{loss_record['model']}: Train Loss: {loss_record['train_loss']:.4f}, Val Loss: {loss_record['val_loss']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "kTK0arKGHtLc",
        "outputId": "e216dc01-a2f7-4c98-a8a7-04eeb98cc295"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.ConditionalVAERNN'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a28eb991882a>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mjoint_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mesm_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cell_input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'protein_input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mrecon_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKLD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mesm_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-fe4a9b412fcb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, joint_embedding)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mjoint_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z (reparameterize) shape:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-fe4a9b412fcb>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, joint_embedding)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_embedding\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use the last hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Take the last layer's hidden state, shape: [batch_size, rnn_hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_encode_mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_encode_logvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RNN_TANH'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    554\u001b[0m                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                                       self.batch_first)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacty of 39.56 GiB of which 40.81 MiB is free. Process 544339 has 39.52 GiB memory in use. Of the allocated memory 38.99 GiB is allocated by PyTorch, and 19.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear memory\n",
        "del model, optimizer, scheduler\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tllWc2e4QSp",
        "outputId": "dcbdb243-a6b0-4b24-bc02-e3849ec4c833"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MhwWmCyQ4Uk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "4EAZTFWkJsPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "vaF9hL2gJxHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}